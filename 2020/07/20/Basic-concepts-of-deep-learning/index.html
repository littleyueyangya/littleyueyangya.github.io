<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><meta name="generator" content="Hexo 4.2.1"><meta name="theme" content="hexo-theme-yun"><title>Basic concepts of deep learning | Hexo</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="none" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.10/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_stqaphw3j4.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link rel="shortcut icon" type="image/svg+xml" href="/yun.svg"><link rel="mask-icon" href="/yun.svg" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"root":"/","title":"云游君的小站","version":"0.9.2","anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><meta name="description" content="该系列主要是参考知乎专栏http:&#x2F;&#x2F;t.cn&#x2F;EL6QPsx ， 为了印象深刻，所以自己再梳理一下，大部分还是参考原文的 基本概念 表示学习（representation learning）：机器学习目的是自动地学到从数据的表示（representation）到数据的标记（label）的映射。随着机器学习算法的日趋成熟，人们发现，在某些领域（如图像、语音、文本等），如何从数据中提取合适的表示成为">
<meta property="og:type" content="article">
<meta property="og:title" content="Basic concepts of deep learning">
<meta property="og:url" content="https://littleyueyangya.github.io/2020/07/20/Basic-concepts-of-deep-learning/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="该系列主要是参考知乎专栏http:&#x2F;&#x2F;t.cn&#x2F;EL6QPsx ， 为了印象深刻，所以自己再梳理一下，大部分还是参考原文的 基本概念 表示学习（representation learning）：机器学习目的是自动地学到从数据的表示（representation）到数据的标记（label）的映射。随着机器学习算法的日趋成熟，人们发现，在某些领域（如图像、语音、文本等），如何从数据中提取合适的表示成为">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://littleyueyangya.github.io/2020/07/20/Basic-concepts-of-deep-learning/1.png">
<meta property="article:published_time" content="2020-07-20T11:26:42.000Z">
<meta property="article:modified_time" content="2020-07-20T11:29:42.274Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://littleyueyangya.github.io/2020/07/20/Basic-concepts-of-deep-learning/1.png"></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script defer src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="Table of Contents"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="Overview"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="John Doe"><img width="96" loading="lazy" src="/Yun.png" alt="John Doe"></a><div class="site-author-name"><a href="/about/">John Doe</a></div><a class="site-name" href="/about/site.html">Hexo</a><sub class="site-subtitle"></sub><div class="site-desciption"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="Home"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="Archives"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">8</span></a></div><div class="site-state-item"><a href="/categories/" title="Categories"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">3</span></a></div><div class="site-state-item"><a href="/tags/" title="Tags"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">5</span></a></div><a class="site-state-item hty-icon-button" href="https://yun.yunyoujun.cn" target="_blank" rel="noopener" title="文档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://wpa.qq.com/msgrd?v=3&amp;uin=910426929&amp;site=qq&amp;menu=yes" title="QQ" target="_blank" style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/YunYouJun" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://weibo.com/jizhideyunyoujun" title="微博" target="_blank" style="color:#E6162D"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-weibo-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.douban.com/people/yunyoujun/" title="豆瓣" target="_blank" style="color:#007722"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-douban-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=247102977" title="网易云音乐" target="_blank" style="color:#C20C0C"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-netease-cloud-music-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.zhihu.com/people/yunyoujun/" title="知乎" target="_blank" style="color:#0084FF"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhihu-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://space.bilibili.com/1579790" title="哔哩哔哩" target="_blank" style="color:#FF8EB3"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-bilibili-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://twitter.com/YunYouJun" title="Twitter" target="_blank" style="color:#1da1f2"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-twitter-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://t.me/elpsycn" title="Telegram Channel" target="_blank" style="color:#0088CC"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-telegram-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:me@yunyoujun.cn" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://travellings.now.sh/" title="Travelling" target="_blank" style="color:var(--hty-text-color)"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-send-plane-2-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a></div></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#正则化（regularization）"><span class="toc-number">1.</span> <span class="toc-text">正则化（regularization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#"><span class="toc-number">2.</span> <span class="toc-text">** **</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现技巧"><span class="toc-number">3.</span> <span class="toc-text">实现技巧</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献"><span class="toc-number">4.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://littleyueyangya.github.io/2020/07/20/Basic-concepts-of-deep-learning/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="John Doe"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="Hexo"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Basic concepts of deep learning</h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="Created: 2020-07-20 19:26:42" itemprop="dateCreated datePublished" datetime="2020-07-20T19:26:42+08:00">2020-07-20</time></div><div class="post-classify"><span class="post-category"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span> <span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">文献阅读</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">计算机视觉</span></a><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><p>该系列主要是参考知乎专栏<a href="http://t.cn/EL6QPsx" target="_blank" rel="noopener">http://t.cn/EL6QPsx</a> ，</p>
<p>为了印象深刻，所以自己再梳理一下，大部分还是参考原文的</p>
<p><strong>基本概念</strong></p>
<p><strong>表示学习（representation learning）：</strong>机器学习目的是自动地学到从数据的表示（representation）到数据的标记（label）的映射。随着机器学习算法的日趋成熟，人们发现，在某些领域（如图像、语音、文本等），如何从数据中提取合适的表示成为整个任务的瓶颈所在，而数据表示的好坏直接影响后续学习任务（所谓garbage in，garbage out）。与其依赖人类专家设计手工特征（难设计还不见得好用），表示学习希望能从数据中自动地学到从数据的原始形式到数据的表示之间的映射。</p>
<p><strong>深度学习（deep learning，DL）：</strong>表示学习的理想很丰满，但实际中人们发现从数据的原始形式直接学得数据表示这件事很难。深度学习是目前最成功的表示学习方法，因此，目前国际表示学习大会（ICLR）的绝大部分论文都是关于深度学习的。深度学习是把表示学习的任务划分成几个小目标，先从数据的原始形式中先学习比较低级的表示，再从低级表示学得比较高级的表示。这样，每个小目标比较容易达到，综合起来我们就完成表示学习的任务。这类似于算法设计思想中的分治法（divide-and-conquer）。</p>
<p><strong>深度神经网络（deep neural networks，DNN）</strong> 深度学习目前几乎唯一行之有效的实现形式。简单的说，深度神经网络就是很深的神经网络。我们利用网络中逐层对特征进行加工的特性，逐渐从低级特征提取高级特征。除了深度神经网络之外，有学者在探索其他深度学习的实现形式，比如深度森林。</p>
<p>深度神经网络目前的成功取决于三大推动因素。1. <strong>大数据</strong>。当数据量小时，很难从数据中学得合适的表示，而传统算法+特征工程往往能取得很好的效果；2. <strong>计算能力</strong>。大的数据和大的网络需要有足够的快的计算能力才能使得模型的应用成为可能。3. <strong>算法创新</strong>。现在很多算法设计关注在如何使网络更好地训练、更快地运行、取得更好的性能。</p>
<p><strong>多层感知机（multi-layer perceptrons，MLP）</strong> 多层由全连接层组成的深度神经网络。多层感知机的最后一层全连接层实质上是一个线性分类器，而其他部分则是为这个线性分类器学习一个合适的数据表示，使倒数第二层的特征线性可分。</p>
<p><strong>激活函数（activation function）</strong>神经网络的必要组成部分。如果没有激活函数，多次线性运算的堆叠仍然是一个线性运算，即不管用再多层实质只起到了一层神经网络的作用。一个好的激活函数应满足以下性质。1. <strong>不会饱和</strong>。sigmoid和tanh激活函数在两侧尾端会有饱和现象，这会使导数在这些区域接近零，从而阻碍网络的训练。2. <strong>零均值</strong>。ReLU激活函数的输出均值不为零，这会影响网络的训练。3. <strong>容易计算</strong>。</p>
<p><strong>迁移学习（transfer learning）</strong> 深度学习下的迁移学习旨在利用源任务数据辅助目标任务数据下的学习。迁移学习适用于源任务数据比目标任务数据多，并且源任务中学习得到的低层特征可以帮助目标任务的学习的情形。在计算机视觉领域，最常用的源任务数据是ImageNet。对ImageNet预训练模型的利用通常有两种方式。1. <strong>固定特征提取器</strong>。用ImageNett预训练模型提取目标任务数据的高层特征。2. <strong>微调（fine-tuning）</strong>。以ImageNet预训练模型作为目标任务模型的初始化初始化权值，之后在目标任务数据上进行微调。</p>
<p><strong>多任务学习（multi-task learning）</strong> 与其针对每个任务训练一个小网络，深度学习下的多任务学习旨在训练一个大网络以同时完成全部任务。这些任务中用于提取低层特征的层是共享的，之后产生分支，各任务拥有各自的若干层用于完成其任务。多任务学习适用于多个任务共享低层特征，并且各个任务的数据很相似的情况。</p>
<p><strong>端到端学习（end-to-end learning）</strong> 深度学习下的端到端学习旨在通过一个深度神经网络直接学习从数据的原始形式到数据的标记的映射。端到端学习并不应该作为我们的一个追求目标，是否要采用端到端学习的一个重要考虑因素是：有没有足够的数据对应端到端的过程，以及我们有没有一些领域知识能够用于整个系统中的一些模块。</p>
<p><strong>优化算法</strong></p>
<p><strong>梯度下降（gradient descent，GD）</strong> 想象你去野足但却迷了路，在漆黑的深夜你一个人被困住山谷中，你知道谷底是出口但是天太黑了根本看不清楚路。于是你确定采取一个贪心(greedy)算法：先试探在当前位置往哪个方向走下降最快（即梯度方向），再朝着这个方向走一小步，重复这个过程直到你到达谷底。这就是梯度下降的基本思想。</p>
<p>梯度下降算法的性能大致取决于三个因素。1. <strong>初始位置</strong>。如果你初始位置就离谷底很近，自然很容易走到谷底。2. <strong>山谷地形</strong>。如果山谷是“九曲十八弯”，很有可能你在里面绕半天都绕不出来。3. <strong>步长</strong>。你每步迈多大，当你步子迈太小，很可能你走半天也没走多远，而当你步子迈太大，一不小心就容易撞到旁边的悬崖峭壁，或者错过了谷底。</p>
<p><strong>误差反向传播（error back-propagation，BP）</strong> 结合微积分中链式法则和算法设计中动态规划思想用于计算梯度。 直接用纸笔推导出中间某一层的梯度的数学表达式是很困难的，但链式法则告诉我们，一旦我们知道后一层的梯度，再结合后一层对当前层的导数，我们就可以得到当前层的梯度。动态规划是一个高效计算所有梯度的实现技巧，通过由高层往低层逐层计算梯度，避免了对高层梯度的重复计算。</p>
<p><strong>滑动平均（moving average）</strong> 要前进的方向不再由当前梯度方向完全决定，而是最近几次梯度方向的滑动平均。利用滑动平均思想的优化算法有带动量（momentum）的SGD、Nesterov动量、Adam（ADAptive Momentum estimation）等。</p>
<p><strong>自适应步长</strong> 自适应地确定权值每一维的步长。当某一维持续震荡时，我们希望这一维的步长小一些；当某一维一直沿着相同的方向前进时，我们希望这一维的步长大一些。利用自适应步长思想的优化算法有AdaGrad、RMSProp、Adam等。</p>
<p><strong>学习率衰减</strong> 当开始训练时，较大的学习率可以使你在参数空间有更大范围的探索；当优化接近收敛时，我们需要小一些的学习率使权值更接近局部最优点。</p>
<p><strong>深度神经网络优化的困难</strong> 有学者指出，在很高维的空间中，局部最优是比较少的，而大部分梯度为零的点是鞍点。平原区域的鞍点会使梯度在很长一段时间内都接近零，这会使得拖慢优化过程。</p>
<p><strong>初始化</strong></p>
<p><strong>初始化的基本思想</strong> 方差不变，即设法对权值进行初始化，使得各层神经元的方差保持不变。</p>
<p><strong>Xavier初始化 ：</strong>从高斯分布或均匀分布中对权值进行采样，使得权值的方差是1/n，其中n是输入神经元的个数，该推导假设激活函数是线性的。</p>
<p><strong>He初始化/MSRA初始化 ：</strong>从高斯分布或均匀分布中对权值进行采样，使得权值的方差是2/n，该推导假设激活函数是ReLU. 导致ReLU会将小于0的神经元置零，大致上会使一半的神经元置零，所以未来弥补丢失的这部分信息，方差要乘以2.</p>
<p><strong>批量规范化（batch-normalization，BN）：</strong>每层显式地对神经元的激活值做规范化，使其具有零均值和单位方差。批量规范化使激活值的分布固定下来，这样可以使得各层更加独立地进行学习。批量规范化可以使得网络对初始化和学习率不太敏感。此外，批量规范化有些许正则化的作用，但不要用其作为正则化手段</p>
<p><strong>偏差/方差（bias/variance）</strong></p>
<p>优化完成后，你发现网络的表现不尽如人意，这时诊断网络处于高偏差/高方差状态是对你下一步调参方向的重要指导。与经典机器学习算法有所不同，因为深度神经网络通常要处理非常高维的特征，所以网络可能同时处于高偏差/高方差的状态，即在特征空间的一些区域网络处于高偏差，而在另一些区域处于高方差。本节，我们对偏差/方差作一简要介绍。</p>
<p><img src="/2020/07/20/Basic-concepts-of-deep-learning/1.png" alt="img" loading="lazy"></p>
<p><strong>偏差</strong> 偏差度量了网络的训练集误差和贝叶斯误差（即能达到的最优误差）的差距。高偏差的网络有很高的训练集误差，说明网络对数据中隐含的一般规律还没有学好。当网络处于高偏差时，通常有以下几种解决方案。<strong>1. 训练更大的网络</strong>。网络越大，对数据潜在规律的拟合能力越强。<strong>2. 更多的训练轮数</strong>。通常训练时间越久，对训练集的拟合能力越强。<strong>3. 改变网络结构</strong>。不同的网络结构对训练集的拟合能力有所不同。</p>
<p><strong>方差</strong> 方差度量了网络的验证集误差和训练集误差的差距。高方差的网络学习能力太强，把训练集中自身独有的一些特点也当作一般规律学得，使网络不能很好的泛化（generalize）到验证集。当网络处于高方差时，通常有以下几种解决方案。<strong>1. 更多的数据</strong>。这是对高方差问题最行之有效的解决方案。<strong>2. 正则化</strong>。<strong>3. 改变网络结构</strong>。不同的网络结构对方差也会有影响。</p>
<h2 id="正则化（regularization）"><a href="#正则化（regularization）" class="headerlink" title="正则化（regularization）"></a><strong>正则化（regularization）</strong></h2><p>正则化是解决高方差问题的重要方案之一。</p>
<p><strong>正则化的基本思想</strong> 正则化的基本思想是使网络的有效大小变小。网络变小之后，网络的拟合能力随之降低，这会使网络不容易过拟合到训练集。</p>
<p><strong>L2正则化</strong> L2正则化倾向于使网络的权值接近0。这会使前一层神经元对后一层神经元的影响降低，使网络变得简单，降低网络的有效大小，降低网络的拟合能力。L2正则化实质上是对权值做线性衰减，所以L2正则化也被称为权值衰减（weight decay）。</p>
<p><strong>随机失活（dropout）</strong> 在训练时，随机失活随机选择一部分神经元，使其置零，不参与本次优化迭代。随机失活减少了每次参与优化迭代的神经元数目，使网络的有效大小变小。随机失活的作用有两点。<strong>1. 降低神经元之间耦合</strong>。因为神经元会被随机置零，所以每个神经元不能依赖于其他神经元，这会迫使每个神经元自身要能提取到合适的特征。<strong>2. 网络集成</strong>。随机失活可以看作在训练时每次迭代定义出一个新的网络，这些网络共享权值。在测试时的网络是这些网络的集成。</p>
<p><strong>数据扩充（data augmentation）</strong> 这实质是获得更多数据的方法。当收集数据很昂贵，或者我们拿到的是第二手数据，数据就这么多时，我们从现有数据中扩充生成更多数据，用生成的“伪造”数据当作更多的真实数据进行训练。以图像数据做分类任务为例，把图像水平翻转、移动一定位置、旋转一定角度、或做一点色彩变化等，这些操作通常都不会影响这幅图像对应的标记。并且你可以尝试这些操作的组合，理论上讲，你可以通过这些组合得到无穷多的训练样本。</p>
<p><strong>早停（early stopping）</strong> 随着训练的进行，当你发现验证集误差不再变化或者开始上升时，提前停止训练。</p>
<p><strong>调参技巧</strong>深度神经网络涉及很多的超参数，如学习率大小、L2正则化系数、动量大小、批量大小、隐层神经元数目、层数、学习率衰减率等。</p>
<p><strong>随机搜索</strong> 由于你事先并不知道哪些超参数对你的问题更重要，因此随机搜索通常是比网格搜索（grid search）更有效的调参策略。</p>
<p><strong>对数空间搜索</strong> 对于隐层神经元数目和层数，可以直接从均匀分布采样进行搜索。而对于学习率、L2正则化系数、和动量，在对数空间搜索更加有效。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import random learning_rate &#x3D; 10 ** random.uniform(-5, -1)  # From 1e-5 to 1e-1 weight_decay &#x3D; 10 ** random.uniform(-7, -1)   # From 1e-7 to 1e-1 momentum &#x3D; 1 - 10 ** random.uniform(-3, -1)   # From 0.9 to 0.999</span><br></pre></td></tr></table></figure>

<h2 id><a href="#" class="headerlink" title="** **"></a>** **</h2><h2 id="实现技巧"><a href="#实现技巧" class="headerlink" title="实现技巧"></a><strong>实现技巧</strong></h2><p><strong>图形处理单元（graphics processing units, GPU）</strong> 深度神经网络的高效实现工具。简单来说，CPU擅长串行、复杂的运算，而GPU擅长并行、简单的运算。深度神经网络中的矩阵运算都十分简单，但计算量巨大。因此，GPU无疑具有非常强大的优势。</p>
<p><strong>向量化（vectorization）</strong> 代码提速的基本技巧。能少写一个for循环就少写一个，能少做一次矩阵运算就少做一次。实质是尽量将多次标量运算转化为一次向量运算；将多次向量运算转化为一次矩阵运算。因为矩阵运算可以并行，这将会比多次单独运算快很多。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><strong>参考文献</strong></h2><ol>
<li><p>Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. Advances in optimizing recurrent networks. In proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 8624–8628, 2013.</p>
</li>
<li><p>J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(12):281–305, 2012.</p>
</li>
<li><p>A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In proceedings of the International Conference on Artificial Intelligence and Statistics, pages 192–204, 2015.</p>
</li>
<li><p>P. Domingos. A few useful things to know about machine learning. Communications of the ACM, 55(10):78–87, 2012.</p>
</li>
<li><p>J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(12):2121–2159, 2011.</p>
</li>
<li><p>I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT press, 2016.</p>
</li>
<li><p>K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In proceedings of the IEEE International Conference on Computer Vision, pages 1026–1034, 2015.</p>
</li>
<li><p>S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In proceedings of the International Conference on Machine Learning, pages 448-456, 2015.</p>
</li>
<li><p>D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
</li>
<li><p>F.-F. Li, A. Karpathy, and J. Johnson. CS231n: Convolutional Neural Networks for Visual Recognition. Stanford, 2016.</p>
</li>
<li><p>F.-F. Li, J. Johnson, and S. Yeung. CS231n: Convolutional Neural Networks for Visual Recognition. Stanford, 2017.</p>
</li>
<li><p>A. Ng. Machine learning yearning. Draft, 2016.</p>
</li>
<li><p>A. Ng. CS229: Machine learning. Stanford.</p>
</li>
<li><p>A. Ng. Neural networks and deep learning. <a href="https://link.zhihu.com/?target=http%3A//deeplearning.ai/">deeplearning.ai</a>.</p>
</li>
<li><p>A. Ng. Improving deep neural networks: Hyperparameter tuning, regularization and optimization. <a href="https://link.zhihu.com/?target=http%3A//deeplearning.ai/">deeplearning.ai</a>.</p>
</li>
<li><p>A. Ng. Structuring machine learning projects. <a href="https://link.zhihu.com/?target=http%3A//deeplearning.ai/">deeplearning.ai</a>.</p>
</li>
<li><p>S Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017.</p>
</li>
<li><p>N. Srivastava, G. E Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929–1958, 2014.</p>
</li>
<li><p>T. Tieleman and G. Hinton. Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.</p>
</li>
<li><p>G. Xavier and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In proceedings of the International Conference on Artificial Intelligence and Statistics, pages 249–256, 2010.</p>
</li>
<li><p>J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In proceedings of Advances in neural information processing systems, pages 3320–3328, 2014.</p>
</li>
<li><p>Z.-H. Zhou and J. Feng. Deep forest: Towards an alternative to deep neural networks. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 3553–3559, 2017.</p>
</li>
<li><p>周志华. 机器学习. 清华大学出版社, 2016.</p>
<p>来源： <a href="https://zhuanlan.zhihu.com/p/31561570" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31561570</a></p>
</li>
</ol>
</div><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="Donate" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span><div id="reward-comment">I'm so cute. Please give me money.</div><div id="qr" style="display:none;"><div style="display:inline-block"><a href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/alipay-qrcode.jpg" target="_blank" rel="noopener"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/alipay-qrcode.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-alipay-line"></use></svg></span></div></div><div style="display:inline-block"><a href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/qqpay-qrcode.png" target="_blank" rel="noopener"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/qqpay-qrcode.png" alt="QQ 支付" title="QQ 支付"></a><div><span style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></span></div></div><div style="display:inline-block"><a href="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/wechatpay-qrcode.jpg" target="_blank" rel="noopener"><img loading="lazy" src="https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/donate/wechatpay-qrcode.jpg" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-pay-line"></use></svg></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>John Doe</li><li class="post-copyright-link"><strong>Post link: </strong><a href="https://littleyueyangya.github.io/2020/07/20/Basic-concepts-of-deep-learning/" title="Basic concepts of deep learning">https://littleyueyangya.github.io/2020/07/20/Basic-concepts-of-deep-learning/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> unless stating additionally.</li></ul></section></article><div class="post-nav"><div class="post-nav-item"><a class="post-nav-prev" href="/2020/07/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%9F%BA%E6%9C%AC%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D/" rel="prev" title="计算机视觉基本任务介绍"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-left-s-line"></use></svg><span class="post-nav-text">计算机视觉基本任务介绍</span></a></div><div class="post-nav-item"><a class="post-nav-next" href="/2020/07/13/image-upload/" rel="next" title="image upload"><span class="post-nav-text">image upload</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div id="comment"><div class="comment-tooltip text-center"><span>点击按钮跳转 GitHub Issues 评论。</span><br><span>若没有本文 Issue，您可以使用 Comment 模版新建。</span><br><a class="hty-button hty-button--raised" id="github-issues" href="https://github.com/YunYouJun/yunyoujun.github.io/issues?q=is:issue+Basic concepts of deep learning" target="_blank" rel="noopener">GitHub Issues</a></div></div></main><footer class="sidebar-translate" id="footer"><div class="copyright"><span>&copy; 2019 – 2020 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> John Doe</span></div><div class="powered"><span>Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> v4.2.1</span><span class="footer-separator">|</span><span>Theme - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v0.9.2</span></div></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a></div><script defer src="/js/utils.js"></script><script defer src="/js/hexo-theme-yun.js"></script></body></html>